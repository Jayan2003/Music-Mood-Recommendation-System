{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install kaggle opencv-python gradio scikit-learn"
      ],
      "metadata": {
        "id": "lWqcNAcBepb-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "!cp kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "print(\"Kaggle configured ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxBeKJ3terSs",
        "outputId": "de791861-4d57-4b7f-9118-357354e02941"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle configured ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d shuvoalok/raf-db-dataset -p /content --force\n",
        "!unzip -q /content/raf-db-dataset.zip -d /content/raf_db\n",
        "\n",
        "print(\"Unzipped ✅\")\n",
        "!ls -la /content/raf_db | head -n 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLfA7oSues_m",
        "outputId": "6e2f6b3d-206b-4da7-9cfc-a0e21c9d5caa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/shuvoalok/raf-db-dataset\n",
            "License(s): other\n",
            "Downloading raf-db-dataset.zip to /content\n",
            "  0% 0.00/37.7M [00:00<?, ?B/s]\n",
            "100% 37.7M/37.7M [00:00<00:00, 1.29GB/s]\n",
            "Unzipped ✅\n",
            "total 412\n",
            "drwxr-xr-x 3 root root   4096 Jan  8 17:00 .\n",
            "drwxr-xr-x 1 root root   4096 Jan  8 16:59 ..\n",
            "drwxr-xr-x 4 root root   4096 Jan  8 17:00 DATASET\n",
            "-rw-r--r-- 1 root root  76713 Sep 20  2023 test_labels.csv\n",
            "-rw-r--r-- 1 root root 331330 Sep 20  2023 train_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "SRC = Path(\"/content/raf_db/DATASET\")\n",
        "TRAIN_ROOT = SRC / \"train\"\n",
        "TEST_ROOT  = SRC / \"test\"\n",
        "\n",
        "assert TRAIN_ROOT.exists(), f\"Missing: {TRAIN_ROOT}\"\n",
        "assert TEST_ROOT.exists(),  f\"Missing: {TEST_ROOT}\"\n",
        "\n",
        "print(\"SRC       :\", SRC)\n",
        "print(\"TRAIN_ROOT:\", TRAIN_ROOT)\n",
        "print(\"TEST_ROOT :\", TEST_ROOT)\n",
        "\n",
        "print(\"\\nTrain folders:\", [p.name for p in TRAIN_ROOT.iterdir() if p.is_dir()])\n",
        "print(\"Test  folders:\", [p.name for p in TEST_ROOT.iterdir() if p.is_dir()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdJPbedIeugW",
        "outputId": "05ee58d5-a372-4bb4-b873-a9192e1054e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC       : /content/raf_db/DATASET\n",
            "TRAIN_ROOT: /content/raf_db/DATASET/train\n",
            "TEST_ROOT : /content/raf_db/DATASET/test\n",
            "\n",
            "Train folders: ['6', '3', '5', '4', '1', '2', '7']\n",
            "Test  folders: ['6', '3', '5', '4', '1', '2', '7']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, random\n",
        "from pathlib import Path\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "OUT = Path(\"/content/dataset_3class\")\n",
        "\n",
        "# Clean output\n",
        "if OUT.exists():\n",
        "    shutil.rmtree(OUT)\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    for cls in [\"happy\", \"sad\", \"neutral\"]:\n",
        "        (OUT / split / cls).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "img_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
        "\n",
        "def list_images(folder: Path):\n",
        "    return [p for p in folder.glob(\"*\") if p.is_file() and p.suffix.lower() in img_exts]\n",
        "\n",
        "# RAF numeric label folders:\n",
        "# 4 = happy, 5 = sad, 7 = neutral\n",
        "FOLDER_TO_CLASS = {\"4\": \"happy\", \"5\": \"sad\", \"7\": \"neutral\"}\n",
        "\n",
        "def copy_split_numeric(split_root: Path, split_name: str):\n",
        "    found_dirs = [p.name for p in split_root.iterdir() if p.is_dir()]\n",
        "    print(f\"{split_name} folders found:\", found_dirs)\n",
        "\n",
        "    counts = {\"happy\": 0, \"sad\": 0, \"neutral\": 0}\n",
        "\n",
        "    for folder_name, cls in FOLDER_TO_CLASS.items():\n",
        "        src_folder = split_root / folder_name\n",
        "        if not src_folder.exists():\n",
        "            raise RuntimeError(f\"Missing folder {src_folder}. Found: {found_dirs}\")\n",
        "\n",
        "        imgs = list_images(src_folder)\n",
        "        counts[cls] = len(imgs)\n",
        "\n",
        "        for img in imgs:\n",
        "            dst = OUT / split_name / cls / img.name\n",
        "            shutil.copy2(img, dst)\n",
        "\n",
        "    return counts\n",
        "\n",
        "train_counts = copy_split_numeric(TRAIN_ROOT, \"train\")\n",
        "test_counts  = copy_split_numeric(TEST_ROOT,  \"test\")\n",
        "\n",
        "print(\"✅ Copied numeric folders\")\n",
        "print(\"Train counts:\", train_counts)\n",
        "print(\"Test counts :\", test_counts)\n",
        "\n",
        "# Create validation split from train (15% per class)\n",
        "for cls in [\"happy\", \"sad\", \"neutral\"]:\n",
        "    train_cls = OUT / \"train\" / cls\n",
        "    val_cls   = OUT / \"val\" / cls\n",
        "\n",
        "    imgs = [p for p in train_cls.glob(\"*\") if p.is_file()]\n",
        "    random.shuffle(imgs)\n",
        "\n",
        "    take = max(1, int(0.15 * len(imgs)))\n",
        "    for p in imgs[:take]:\n",
        "        p.rename(val_cls / p.name)\n",
        "\n",
        "print(\"\\n✅ Final dataset sizes:\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    print(split, {cls: len(list((OUT/split/cls).glob(\"*\"))) for cls in [\"happy\",\"sad\",\"neutral\"]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LZyumlJewef",
        "outputId": "4f4d57f6-3fb0-453b-9ec2-de07d184dfa8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train folders found: ['6', '3', '5', '4', '1', '2', '7']\n",
            "test folders found: ['6', '3', '5', '4', '1', '2', '7']\n",
            "✅ Copied numeric folders\n",
            "Train counts: {'happy': 4772, 'sad': 1982, 'neutral': 2524}\n",
            "Test counts : {'happy': 1185, 'sad': 478, 'neutral': 680}\n",
            "\n",
            "✅ Final dataset sizes:\n",
            "train {'happy': 4057, 'sad': 1685, 'neutral': 2146}\n",
            "val {'happy': 715, 'sad': 297, 'neutral': 378}\n",
            "test {'happy': 1185, 'sad': 478, 'neutral': 680}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "def balance_train():\n",
        "    split_folder = OUT / \"train\"\n",
        "    files = {cls: [p for p in (split_folder/cls).glob(\"*\") if p.is_file()]\n",
        "             for cls in [\"happy\",\"sad\",\"neutral\"]}\n",
        "    counts0 = {k: len(v) for k, v in files.items()}\n",
        "    m = min(counts0.values())\n",
        "    print(\"Before balancing:\", counts0, \"-> target:\", m)\n",
        "\n",
        "    for cls, lst in files.items():\n",
        "        if len(lst) > m:\n",
        "            for p in lst[m:]:\n",
        "                p.unlink()\n",
        "\n",
        "    counts1 = {cls: len(list((split_folder/cls).glob(\"*\"))) for cls in [\"happy\",\"sad\",\"neutral\"]}\n",
        "    print(\"After balancing:\", counts1)\n",
        "\n",
        "balance_train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwlwmMwAe1Sq",
        "outputId": "d66f84c0-b8e6-476b-9065-2e978b0ec454"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before balancing: {'happy': 4057, 'sad': 1685, 'neutral': 2146} -> target: 1685\n",
            "After balancing: {'happy': 1685, 'sad': 1685, 'neutral': 1685}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = (48, 48)\n",
        "BATCH = 64\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "CLASS_NAMES = [\"happy\", \"sad\", \"neutral\"]\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    OUT / \"train\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=CLASS_NAMES,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=True,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    OUT / \"val\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=CLASS_NAMES,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    OUT / \"test\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=CLASS_NAMES,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "def normalize(x, y):\n",
        "    x = tf.cast(x, tf.float32) / 255.0\n",
        "    return x, y\n",
        "\n",
        "train_ds = train_ds.map(normalize, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
        "val_ds   = val_ds.map(normalize,   num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
        "test_ds  = test_ds.map(normalize,  num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
        "\n",
        "print(\"Pipelines ready ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWYUPRdOe3Qr",
        "outputId": "7150b4fa-8dbf-4382-8e26-034376ca4709"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5055 files belonging to 3 classes.\n",
            "Found 1390 files belonging to 3 classes.\n",
            "Found 2343 files belonging to 3 classes.\n",
            "Pipelines ready ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data_aug = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.1),\n",
        "], name=\"data_aug\")\n",
        "\n",
        "cnn = keras.Sequential([\n",
        "    layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1)),\n",
        "    data_aug,\n",
        "\n",
        "    layers.Conv2D(32, 3, padding=\"same\"), layers.BatchNormalization(), layers.ReLU(),\n",
        "    layers.MaxPool2D(),\n",
        "\n",
        "    layers.Conv2D(64, 3, padding=\"same\"), layers.BatchNormalization(), layers.ReLU(),\n",
        "    layers.MaxPool2D(),\n",
        "\n",
        "    layers.Conv2D(128, 3, padding=\"same\"), layers.BatchNormalization(), layers.ReLU(),\n",
        "    layers.MaxPool2D(),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation=\"relu\", name=\"embedding\"),\n",
        "    layers.Dropout(0.4),\n",
        "\n",
        "    layers.Dense(3, activation=\"softmax\", name=\"emotion_head\"),\n",
        "], name=\"cnn_emotion\")\n",
        "\n",
        "cnn.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "cnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "WHCwBVe-e7tp",
        "outputId": "fd5f87cd-7b97-4378-8ce6-1e4c54fe2ccc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cnn_emotion\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_emotion\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ data_aug (\u001b[38;5;33mSequential\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,179,904\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ emotion_head (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m771\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ data_aug (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ emotion_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">771</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,274,243\u001b[0m (4.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,274,243</span> (4.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,273,795\u001b[0m (4.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,273,795</span> (4.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "\n",
        "SAVE_DIR = \"/content/raf3_emotion_project\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BEST_CNN_PATH = os.path.join(SAVE_DIR, \"best_cnn.keras\")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(BEST_CNN_PATH, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
        "]\n",
        "\n",
        "history = cnn.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=callbacks)\n",
        "print(\"Saved best CNN locally ✅:\", BEST_CNN_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUxQ3KYie-bb",
        "outputId": "deb1bbea-fc8b-4cbc-8544-60d41e4f4194"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3865 - loss: 2.6359\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51439, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 68ms/step - accuracy: 0.3868 - loss: 2.6223 - val_accuracy: 0.5144 - val_loss: 1.0281 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4763 - loss: 1.0259\n",
            "Epoch 2: val_accuracy did not improve from 0.51439\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.4764 - loss: 1.0258 - val_accuracy: 0.5144 - val_loss: 1.0129 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5261 - loss: 0.9530\n",
            "Epoch 3: val_accuracy improved from 0.51439 to 0.51511, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5262 - loss: 0.9529 - val_accuracy: 0.5151 - val_loss: 0.9792 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5497 - loss: 0.9061\n",
            "Epoch 4: val_accuracy improved from 0.51511 to 0.52878, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5497 - loss: 0.9060 - val_accuracy: 0.5288 - val_loss: 0.9313 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5966 - loss: 0.8522\n",
            "Epoch 5: val_accuracy improved from 0.52878 to 0.67554, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5966 - loss: 0.8519 - val_accuracy: 0.6755 - val_loss: 0.8595 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m77/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6031 - loss: 0.8286\n",
            "Epoch 6: val_accuracy improved from 0.67554 to 0.70647, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.6033 - loss: 0.8283 - val_accuracy: 0.7065 - val_loss: 0.6949 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m77/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6145 - loss: 0.8031\n",
            "Epoch 7: val_accuracy improved from 0.70647 to 0.72086, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.6148 - loss: 0.8024 - val_accuracy: 0.7209 - val_loss: 0.6467 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6433 - loss: 0.7684\n",
            "Epoch 8: val_accuracy improved from 0.72086 to 0.73237, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.6434 - loss: 0.7683 - val_accuracy: 0.7324 - val_loss: 0.6469 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m77/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6502 - loss: 0.7462\n",
            "Epoch 9: val_accuracy did not improve from 0.73237\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.6507 - loss: 0.7458 - val_accuracy: 0.7086 - val_loss: 0.7008 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m77/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6739 - loss: 0.7234\n",
            "Epoch 10: val_accuracy improved from 0.73237 to 0.74101, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.6744 - loss: 0.7227 - val_accuracy: 0.7410 - val_loss: 0.5878 - learning_rate: 5.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7004 - loss: 0.6807\n",
            "Epoch 11: val_accuracy improved from 0.74101 to 0.75396, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.7005 - loss: 0.6805 - val_accuracy: 0.7540 - val_loss: 0.5766 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7002 - loss: 0.6691\n",
            "Epoch 12: val_accuracy did not improve from 0.75396\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7002 - loss: 0.6690 - val_accuracy: 0.6532 - val_loss: 0.7053 - learning_rate: 5.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7043 - loss: 0.6491\n",
            "Epoch 13: val_accuracy did not improve from 0.75396\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.7042 - loss: 0.6492 - val_accuracy: 0.7446 - val_loss: 0.6044 - learning_rate: 5.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m77/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7245 - loss: 0.6219\n",
            "Epoch 14: val_accuracy improved from 0.75396 to 0.76763, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7244 - loss: 0.6217 - val_accuracy: 0.7676 - val_loss: 0.5525 - learning_rate: 2.5000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7252 - loss: 0.6231\n",
            "Epoch 15: val_accuracy improved from 0.76763 to 0.80000, saving model to /content/raf3_emotion_project/best_cnn.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7253 - loss: 0.6229 - val_accuracy: 0.8000 - val_loss: 0.4942 - learning_rate: 2.5000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m77/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7356 - loss: 0.6091\n",
            "Epoch 16: val_accuracy did not improve from 0.80000\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7355 - loss: 0.6089 - val_accuracy: 0.7748 - val_loss: 0.5527 - learning_rate: 2.5000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7327 - loss: 0.6194\n",
            "Epoch 17: val_accuracy did not improve from 0.80000\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.7328 - loss: 0.6192 - val_accuracy: 0.7942 - val_loss: 0.4959 - learning_rate: 2.5000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m77/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7462 - loss: 0.5910\n",
            "Epoch 18: val_accuracy did not improve from 0.80000\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7459 - loss: 0.5910 - val_accuracy: 0.7863 - val_loss: 0.5151 - learning_rate: 1.2500e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7417 - loss: 0.5896\n",
            "Epoch 19: val_accuracy did not improve from 0.80000\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7418 - loss: 0.5895 - val_accuracy: 0.7482 - val_loss: 0.5706 - learning_rate: 1.2500e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7669 - loss: 0.5627\n",
            "Epoch 20: val_accuracy did not improve from 0.80000\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7669 - loss: 0.5627 - val_accuracy: 0.7835 - val_loss: 0.5155 - learning_rate: 6.2500e-05\n",
            "Epoch 20: early stopping\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "Saved best CNN locally ✅: /content/raf3_emotion_project/best_cnn.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "cnn_best = keras.models.load_model(BEST_CNN_PATH)\n",
        "\n",
        "loss, acc = cnn_best.evaluate(test_ds, verbose=0)\n",
        "print(\"Test accuracy:\", acc)\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for xb, yb in test_ds:\n",
        "    probs = cnn_best.predict(xb, verbose=0)\n",
        "    y_true.extend(yb.numpy().tolist())\n",
        "    y_pred.extend(np.argmax(probs, axis=1).tolist())\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "pECfYgsPfAvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90dfe435-29ca-41b0-e4c3-14d273228c83"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.7959880232810974\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       happy       0.94      0.84      0.89      1185\n",
            "         sad       0.60      0.75      0.67       478\n",
            "     neutral       0.75      0.75      0.75       680\n",
            "\n",
            "    accuracy                           0.80      2343\n",
            "   macro avg       0.76      0.78      0.77      2343\n",
            "weighted avg       0.81      0.80      0.80      2343\n",
            "\n",
            "Confusion matrix:\n",
            " [[995  95  95]\n",
            " [ 40 359  79]\n",
            " [ 26 143 511]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "def build_caption_bank(n_per_emotion=120, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # building blocks (phrases)\n",
        "    starters = [\n",
        "        \"The face\", \"This person\", \"The expression\", \"The subject\", \"The individual\"\n",
        "    ]\n",
        "\n",
        "    verbs = [\n",
        "        \"shows\", \"displays\", \"reveals\", \"has\", \"presents\"\n",
        "    ]\n",
        "\n",
        "    # emotion-specific phrase pools\n",
        "    emotion_core = {\n",
        "        0: [\"happiness\", \"joy\", \"a happy emotion\", \"a cheerful mood\", \"positive emotion\"],\n",
        "        1: [\"sadness\", \"a sad emotion\", \"a down mood\", \"negative emotion\", \"melancholy\"],\n",
        "        2: [\"a neutral emotion\", \"no strong emotion\", \"a calm mood\", \"a composed look\", \"a neutral state\"]\n",
        "    }\n",
        "\n",
        "    # emotion-specific facial cues\n",
        "    cues = {\n",
        "        0: [\n",
        "            \"with a clear smile\", \"with a bright smile\", \"with lifted cheeks\",\n",
        "            \"with relaxed eyes\", \"with a warm smile\", \"with an upbeat look\",\n",
        "            \"with smiling eyes\", \"with an open, friendly look\"\n",
        "        ],\n",
        "        1: [\n",
        "            \"with a downturned mouth\", \"with heavy eyes\", \"with a tired gaze\",\n",
        "            \"with lowered lips\", \"with reduced facial energy\", \"with a tense look\",\n",
        "            \"with a pained expression\", \"with a strained mouth shape\"\n",
        "        ],\n",
        "        2: [\n",
        "            \"with relaxed facial muscles\", \"with a steady gaze\", \"with a calm look\",\n",
        "            \"with a composed face\", \"with a neutral mouth\", \"with balanced features\",\n",
        "            \"with no strong cues\", \"with a relaxed expression\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # optional ending / extra detail\n",
        "    endings = [\n",
        "        \"\", \".\", \" overall.\", \" clearly.\", \" in the face.\", \" in the eyes and mouth.\"\n",
        "    ]\n",
        "\n",
        "    bank = {0: [], 1: [], 2: []}\n",
        "\n",
        "    for emo in [0, 1, 2]:\n",
        "        # create many combinations then sample\n",
        "        combos = []\n",
        "        for s, v, ec, c, e in itertools.product(starters, verbs, emotion_core[emo], cues[emo], endings):\n",
        "            # Example: \"The face shows happiness with a bright smile overall.\"\n",
        "            combos.append(f\"{s} {v} {ec} {c}{e}\".replace(\"..\", \".\").strip())\n",
        "\n",
        "        # remove duplicates while preserving order\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for x in combos:\n",
        "            x_norm = x.lower()\n",
        "            if x_norm not in seen:\n",
        "                seen.add(x_norm)\n",
        "                unique.append(x)\n",
        "\n",
        "        # sample n_per_emotion captions\n",
        "        if len(unique) < n_per_emotion:\n",
        "            chosen = unique\n",
        "        else:\n",
        "            idx = rng.choice(len(unique), size=n_per_emotion, replace=False)\n",
        "            chosen = [unique[i] for i in idx]\n",
        "\n",
        "        bank[emo] = chosen\n",
        "\n",
        "    return bank\n",
        "\n",
        "caption_bank = build_caption_bank(n_per_emotion=150, seed=42)\n",
        "\n",
        "def make_caption(label_id):\n",
        "    opts = caption_bank[int(label_id)]\n",
        "    return opts[int(rng.integers(0, len(opts)))]\n",
        "\n",
        "print(\"Caption counts:\", {k: len(v) for k, v in caption_bank.items()})\n",
        "print(\"Sample happy captions:\", caption_bank[0][:5])"
      ],
      "metadata": {
        "id": "WePdZu2PfClt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c28d224-d9c4-4b56-c278-07060ec56128"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption counts: {0: 150, 1: 150, 2: 150}\n",
            "Sample happy captions: ['This person presents happiness with a bright smile.', 'The expression has joy with smiling eyes overall.', 'The expression has positive emotion with an upbeat look in the face.', 'The subject presents joy with an open, friendly look in the face.', 'The subject shows a happy emotion with smiling eyes in the eyes and mouth.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = []\n",
        "for _, yb in train_ds:\n",
        "    train_labels.extend(yb.numpy().tolist())\n",
        "\n",
        "val_labels = []\n",
        "for _, yb in val_ds:\n",
        "    val_labels.extend(yb.numpy().tolist())\n",
        "\n",
        "train_captions = [make_caption(y) for y in train_labels]\n",
        "val_captions   = [make_caption(y) for y in val_labels]\n",
        "\n",
        "print(\"Example captions:\", train_captions[:3])"
      ],
      "metadata": {
        "id": "yKNQi6k0fG8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a15b1d2-4e0a-4078-d814-46507c9653c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example captions: ['The individual reveals joy with an upbeat look', 'The subject shows a composed look with a neutral mouth.', 'The individual reveals a calm mood with a neutral mouth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os, json\n",
        "\n",
        "MAX_TOKENS = 2000\n",
        "SEQ_LEN = 24\n",
        "EMB_DIM = 128\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LEN\n",
        ")\n",
        "vectorizer.adapt(train_captions)\n",
        "vocab_size = len(vectorizer.get_vocabulary())\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "def make_lm_arrays(captions, labels):\n",
        "    x = vectorizer(tf.constant(captions))   # [N, SEQ_LEN]\n",
        "    inp = x[:, :-1]                          # [N, T]\n",
        "    tgt = x[:, 1:]                           # [N, T]\n",
        "    emo = np.array(labels, dtype=np.int32)   # [N]\n",
        "    return inp.numpy(), tgt.numpy(), emo\n",
        "\n",
        "xtr, ytr, etr = make_lm_arrays(train_captions, train_labels)\n",
        "xva, yva, eva = make_lm_arrays(val_captions,   val_labels)\n",
        "\n",
        "BATCH_LM = 64\n",
        "train_lm = tf.data.Dataset.from_tensor_slices(((xtr, etr), ytr)).shuffle(2000, seed=42).batch(BATCH_LM)\n",
        "val_lm   = tf.data.Dataset.from_tensor_slices(((xva, eva), yva)).batch(BATCH_LM)\n",
        "\n",
        "token_in = layers.Input(shape=(SEQ_LEN-1,), dtype=tf.int32, name=\"token_in\")\n",
        "emo_in   = layers.Input(shape=(), dtype=tf.int32, name=\"emo_in\")\n",
        "\n",
        "tok_emb = layers.Embedding(vocab_size, EMB_DIM, name=\"tok_emb\")(token_in)\n",
        "emo_emb = layers.Embedding(3, EMB_DIM, name=\"emo_emb\")(emo_in)\n",
        "emo_rep = layers.RepeatVector(SEQ_LEN-1)(emo_emb)\n",
        "\n",
        "x = layers.Add()([tok_emb, emo_rep])\n",
        "x = layers.GRU(256, return_sequences=True)(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "logits = layers.Dense(vocab_size)(x)\n",
        "\n",
        "decoder = keras.Model([token_in, emo_in], logits, name=\"emotion_text_decoder\")\n",
        "decoder.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "decoder.summary()\n",
        "\n",
        "history_dec = decoder.fit(train_lm, validation_data=val_lm, epochs=10)"
      ],
      "metadata": {
        "id": "FNyRUO7WfI0R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "outputId": "d3a293ee-ce96-4e13-9562-e7a869d9b7ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"emotion_text_decoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"emotion_text_decoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ emo_in (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_in            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ emo_emb (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m384\u001b[0m │ emo_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tok_emb (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │      \u001b[38;5;34m9,088\u001b[0m │ token_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ emo_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ tok_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │                   │            │ repeat_vector[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m296,448\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m71\u001b[0m)    │     \u001b[38;5;34m18,247\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ emo_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_in            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ emo_emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │ emo_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tok_emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,088</span> │ token_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ emo_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ tok_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │                   │            │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,247</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m324,167\u001b[0m (1.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">324,167</span> (1.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m324,167\u001b[0m (1.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">324,167</span> (1.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 2.3158 - sparse_categorical_accuracy: 0.5506 - val_loss: 0.8897 - val_sparse_categorical_accuracy: 0.7624\n",
            "Epoch 2/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.6616 - sparse_categorical_accuracy: 0.7958 - val_loss: 0.3766 - val_sparse_categorical_accuracy: 0.8352\n",
            "Epoch 3/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3794 - sparse_categorical_accuracy: 0.8327 - val_loss: 0.3534 - val_sparse_categorical_accuracy: 0.8388\n",
            "Epoch 4/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3601 - sparse_categorical_accuracy: 0.8362 - val_loss: 0.3462 - val_sparse_categorical_accuracy: 0.8400\n",
            "Epoch 5/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3512 - sparse_categorical_accuracy: 0.8391 - val_loss: 0.3438 - val_sparse_categorical_accuracy: 0.8435\n",
            "Epoch 6/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.3494 - sparse_categorical_accuracy: 0.8390 - val_loss: 0.3404 - val_sparse_categorical_accuracy: 0.8423\n",
            "Epoch 7/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.3451 - sparse_categorical_accuracy: 0.8409 - val_loss: 0.3368 - val_sparse_categorical_accuracy: 0.8437\n",
            "Epoch 8/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3413 - sparse_categorical_accuracy: 0.8426 - val_loss: 0.3323 - val_sparse_categorical_accuracy: 0.8450\n",
            "Epoch 9/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.3361 - sparse_categorical_accuracy: 0.8447 - val_loss: 0.3287 - val_sparse_categorical_accuracy: 0.8497\n",
            "Epoch 10/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3323 - sparse_categorical_accuracy: 0.8468 - val_loss: 0.3251 - val_sparse_categorical_accuracy: 0.8512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "DECODER_DIR = os.path.join(SAVE_DIR, \"decoder_text\")\n",
        "os.makedirs(DECODER_DIR, exist_ok=True)\n",
        "\n",
        "decoder_path = os.path.join(DECODER_DIR, \"decoder.keras\")\n",
        "decoder.save(decoder_path)\n",
        "\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "with open(os.path.join(DECODER_DIR, \"vocab.json\"), \"w\") as f:\n",
        "    json.dump(vocab, f)\n",
        "\n",
        "print(\"Saved decoder ✅:\", decoder_path)\n",
        "print(\"Saved vocab ✅:\", os.path.join(DECODER_DIR, \"vocab.json\"))\n"
      ],
      "metadata": {
        "id": "6JD7F5YpfNnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69d4944-022d-4ae7-c702-72b20e608828"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved decoder ✅: /content/raf3_emotion_project/decoder_text/decoder.keras\n",
            "Saved vocab ✅: /content/raf3_emotion_project/decoder_text/vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_description(emotion_id, temperature=0.8):\n",
        "    vocab = vectorizer.get_vocabulary()\n",
        "    word_to_id = {w: i for i, w in enumerate(vocab)}\n",
        "\n",
        "    # seed with a real token (not PAD=0)\n",
        "    start_id = word_to_id.get(\"the\", None)\n",
        "    if start_id is None or start_id == 0:\n",
        "        start_id = 1  # fallback (usually [UNK])\n",
        "\n",
        "    cur = np.zeros((1, SEQ_LEN-1), dtype=np.int32)\n",
        "    cur[0, 0] = start_id\n",
        "\n",
        "    for t in range(1, SEQ_LEN-1):\n",
        "        logits = decoder.predict([cur, np.array([emotion_id], dtype=np.int32)], verbose=0)\n",
        "        step_logits = logits[0, t-1] / max(temperature, 1e-6)\n",
        "        probs = tf.nn.softmax(step_logits).numpy()\n",
        "        next_id = int(np.random.choice(len(probs), p=probs))\n",
        "        cur[0, t] = next_id\n",
        "\n",
        "    words = [vocab[i] for i in cur[0] if i != 0]  # remove PAD\n",
        "    return \" \".join(words).strip()\n"
      ],
      "metadata": {
        "id": "P1GBMST3fPTB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "SAVE_DIR = \"/content/raf3_emotion_project\"\n",
        "\n",
        "zip_path = shutil.make_archive(\"/content/raf3_emotion_project_backup\", \"zip\", SAVE_DIR)\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "aYjhODkwsPaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0b60d312-760c-4f26-aa57-f5d04c23252e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_567c4732-4149-45a5-ae68-7d1ffdd9060d\", \"raf3_emotion_project_backup.zip\", 17233785)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os, random\n",
        "\n",
        "# Emotion -> YouTube videos\n",
        "YOUTUBE = {\n",
        "    \"happy\": [\n",
        "        \"https://www.youtube.com/embed/JnCTOi2QAZk?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/w7JBHSx5ScA?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/KZYqugtbcG0?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/T-LAJ0Y7lsw?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/Tl1WS_PvsF4?autoplay=1\",\n",
        "    ],\n",
        "    \"sad\": [\n",
        "        \"https://www.youtube.com/embed/DNZzquw45-k?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/v1rrjiOxiEY?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/cvOgOjRZzr8?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/sfCy1JhCwWg?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/ou-7AboPoXE?autoplay=1\",\n",
        "    ],\n",
        "    \"neutral\": [\n",
        "        \"https://www.youtube.com/embed/h--ykVNBUYQ?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/QHC-9PGT5ZQ?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/-HlEAEqps9c?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/_RHIECWv728?autoplay=1\",\n",
        "        \"https://www.youtube.com/embed/Im74ME1192E?autoplay=1\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "def youtube_iframe(emotion_name: str):\n",
        "    urls = YOUTUBE.get(emotion_name, [])\n",
        "    if not urls:\n",
        "        return \"<p>No recommendations for this emotion.</p>\", \"\"\n",
        "    url = random.choice(urls)\n",
        "    html = f\"\"\"\n",
        "    <div style=\"width:100%; max-width:760px;\">\n",
        "      <iframe width=\"100%\" height=\"420\"\n",
        "        src=\"{url}\"\n",
        "        title=\"YouTube video player\"\n",
        "        frameborder=\"0\"\n",
        "        allow=\"autoplay; encrypted-media\"\n",
        "        allowfullscreen>\n",
        "      </iframe>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    return html, url\n",
        "\n",
        "# Face crop (Haar cascade)\n",
        "CASCADE_PATH = \"/content/haarcascade_frontalface_default.xml\"\n",
        "if not os.path.exists(CASCADE_PATH):\n",
        "    !wget -q https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml -O /content/haarcascade_frontalface_default.xml\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(CASCADE_PATH)\n",
        "\n",
        "def crop_largest_face(gray_img):\n",
        "    faces = face_cascade.detectMultiScale(\n",
        "        gray_img,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=5,\n",
        "        minSize=(40, 40)\n",
        "    )\n",
        "    if len(faces) == 0:\n",
        "        return None, False\n",
        "\n",
        "    x, y, w, h = max(faces, key=lambda b: b[2] * b[3])\n",
        "    pad = int(0.15 * max(w, h))\n",
        "    x1 = max(0, x - pad)\n",
        "    y1 = max(0, y - pad)\n",
        "    x2 = min(gray_img.shape[1], x + w + pad)\n",
        "    y2 = min(gray_img.shape[0], y + h + pad)\n",
        "\n",
        "    return gray_img[y1:y2, x1:x2], True\n",
        "\n",
        "def preprocess_pil_for_cnn(pil_img):\n",
        "    img = np.array(pil_img)  # RGB\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    face, used_crop = crop_largest_face(gray)\n",
        "    if face is None:\n",
        "        face = gray\n",
        "        used_crop = False\n",
        "\n",
        "    face = cv2.resize(face, IMG_SIZE)\n",
        "    x = face.astype(np.float32) / 255.0\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    return x, used_crop\n",
        "\n",
        "# Main inference for Gradio\n",
        "def predict_emotion_and_text(pil_img):\n",
        "    if pil_img is None:\n",
        "        return \"Please upload an image.\", \"\", \"\", \"No image.\", \"<p></p>\", \"\"\n",
        "\n",
        "    x, used_crop = preprocess_pil_for_cnn(pil_img)\n",
        "    probs = cnn_best.predict(x, verbose=0)[0]\n",
        "    idx = int(np.argmax(probs))\n",
        "    emotion = CLASS_NAMES[idx]\n",
        "\n",
        "    desc = generate_description(idx, temperature=0.8)\n",
        "    prob_text = \"\\n\".join([f\"{CLASS_NAMES[i]}: {probs[i]:.4f}\" for i in range(len(CLASS_NAMES))])\n",
        "\n",
        "    info = \"Face crop used ✅ (largest face).\" if used_crop else \"No face detected → used full image.\"\n",
        "\n",
        "    # YouTube recommendation\n",
        "    yt_html, yt_url = youtube_iframe(emotion)\n",
        "\n",
        "    return emotion, prob_text, desc, info, yt_html, yt_url\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_emotion_and_text,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload a face image\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Predicted Emotion\"),\n",
        "        gr.Textbox(label=\"Probabilities\"),\n",
        "        gr.Textbox(label=\"Generated Emotion Description (Decoder-only)\"),\n",
        "        gr.Textbox(label=\"Info\"),\n",
        "        gr.HTML(label=\"Recommended Song (YouTube)\"),\n",
        "        gr.Textbox(label=\"Chosen YouTube URL\"),\n",
        "    ],\n",
        "    title=\"RAF-DB (3 classes) — Face Crop + Emotion + Description + Song\",\n",
        "    description=\"Upload → face crop → CNN emotion (happy/sad/neutral) → decoder description → YouTube song recommendation.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=False)"
      ],
      "metadata": {
        "id": "gD2xo_QqfUwy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "outputId": "d3ddb1d1-3488-4e45-cf27-248aae03766a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d76d29b9982cce3b91.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d76d29b9982cce3b91.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}